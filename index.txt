4:"$Sreact.fragment"
5:I[8126,["676","static/chunks/870fdd6f-e564c4dfd7fd2e96.js","561","static/chunks/561-79e7d1e5757edd0e.js","185","static/chunks/app/layout-1b368952c5244096.js"],"ThemeSwitcher"]
6:I[9275,[],""]
7:I[1343,[],""]
a:I[3120,[],"OutletBoundary"]
c:I[3120,[],"MetadataBoundary"]
e:I[3120,[],"ViewportBoundary"]
10:I[6130,[],""]
1:HL["/llama-homeassistant-blog/_next/static/media/a34f9d1faa5f3315-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
2:HL["/llama-homeassistant-blog/_next/static/css/8809c13df98b528c.css","style"]
3:HL["/llama-homeassistant-blog/_next/static/css/16eaebc046b4020f.css","style"]
8:T518,M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z0:{"P":null,"b":"5CNQrIv-X5bMo_CQ_AsNc","p":"/llama-homeassistant-blog","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$4","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/llama-homeassistant-blog/_next/static/css/8809c13df98b528c.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","type":"image/png","href":"https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png"}],["$","meta",null,{"name":"msapplication-TileColor","content":"#000000"}],["$","meta",null,{"name":"msapplication-config","content":"/favicon/browserconfig.xml"}],["$","meta",null,{"name":"theme-color","content":"#000"}],["$","link",null,{"rel":"alternate","type":"application/rss+xml","href":"/feed.xml"}]]}],["$","body",null,{"className":"__className_a3f648 dark:bg-slate-900 dark:text-slate-400","children":[["$","$L5",null,{}],["$","div",null,{"className":"min-h-screen","children":["$","$L6",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[]}]}],["$","footer",null,{"className":"bg-neutral-50 border-t border-neutral-200 dark:bg-slate-800","children":["$","div",null,{"className":"container mx-auto px-5","children":["$","div",null,{"className":"py-6 flex flex-col lg:flex-row items-center justify-between","children":[["$","h3",null,{"className":"text-xs font-small text-center lg:text-left lg:mb-0 order-2 lg:order-1","children":"Built with Next.js"}],["$","div",null,{"className":"flex flex-row gap-2 mb-4 lg:mb-0 items-center order-1 lg:order-2","children":[["$","svg",null,{"aria-hidden":"true","focusable":"false","data-prefix":"fab","data-icon":"github","className":"svg-inline--fa fa-github w-5 h-5 lg:w-7 lg:h-7 text-neutral-700 dark:text-neutral-200","role":"img","xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 496 512","style":{},"ref":"$undefined","children":["$","path",null,{"fill":"currentColor","d":"$8","style":{}}]}],["$","a",null,{"href":"https://github.com/miguelg719/llama-homeassistant-blog","className":"text-sm hover:underline","children":"GitHub"}]]}]]}]}]}]]}]]}]]}],{"children":["__PAGE__",["$","$4","c",{"children":["$L9",[["$","link","0",{"rel":"stylesheet","href":"/llama-homeassistant-blog/_next/static/css/16eaebc046b4020f.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$La",null,{"children":"$Lb"}]]}],{},null]},null],["$","$4","h",{"children":[null,["$","$4","FZj3jH1cBLDMELvEneZ9c",{"children":[["$","$Lc",null,{"children":"$Ld"}],["$","$Le",null,{"children":"$Lf"}],["$","meta",null,{"name":"next-size-adjust"}]]}]]}]]],"m":"$undefined","G":["$10","$undefined"],"s":false,"S":true}
11:I[7067,["528","static/chunks/528-59fd6bbdde9e2253.js","931","static/chunks/app/page-d765ff1845a6d077.js"],"PostBody"]
12:T20a4,<p>Smart home technology has revolutionized how we interact with our homes, but it comes with significant trade-offs: fragmented ecosystems, privacy concerns, and a lack of truly personalized automation. What if you could have all the convenience of a smart home assistant without sacrificing your data or control? In this demo we introduce a groundbreaking integration of Home Assistant and Meta’s Llama 3.2 models delivering the world’s first private, local, and adaptive smart home assistant.</p>
<p>The current landscape of smart home technology is riddled with challenges:</p>
<ol>
<li><strong>Fragmentation</strong>: Devices from different brands rarely play well together, forcing users to juggle multiple apps and interfaces.</li>
<li><strong>Privacy Concerns</strong>: Popular assistants rely on cloud-based processing, exposing your data to external servers and risking potential breaches.</li>
<li><strong>Lack of Personalization</strong>: While assistants can perform basic tasks, they often fail to learn and adapt to your unique needs and habits.</li>
</ol>
<p>These limitations create a frustrating user experience that prioritizes convenience over security and customization. Open source tools like Home Assistant, an open-source platform that connects and manages smart devices, aim to address some of these issues, but their complex setup and learning curve can deter non-technical users. By leveraging Meta’s Llama 3.2, an advanced large language model (LLM) capable of natural language processing, we’ve created a system that:</p>
<ol>
<li><strong>Processes Locally</strong>: All data stays on your home network, ensuring privacy and eliminating dependence on the cloud.</li>
<li><strong>Simplifies Smart Home Management</strong>: Home Assistant acts as a centralized hub, unifying control of devices across brands and ecosystems whereas the Llama model allows you to control devices and create automations using intuitive voice or text commands.</li>
<li><strong>Learns and Adapts</strong>: The system proactively suggests and implements automations based on your preferences and routines.</li>
</ol>
<h2><strong>Try it yourself</strong></h2>
<p>In this quickstart, we will guide you through the first steps of setting up the integration. We will download Home Assistant as a docker container, run through the onboarding process, start the Ollama server, and finally run a gradio frontend to interact with the Llama model.</p>
<h3><strong>Prerequisites</strong></h3>
<ul>
<li>Docker</li>
<li>Python 3.10+</li>
<li>Ollama</li>
</ul>
<h3><strong>1. Clone the repo</strong></h3>
<p>You can find the repo <strong><a href="https://github.com/miguelg719/homeassistant-llama">here</a></strong>.</p>
<pre><code class="language-bash">git clone https://github.com/miguelg719/homeassistant-llama
cd homeassistant-llama
</code></pre>
<p>Notice how the repo contains both a frontend/ and backend/ directory. The frontend directory contains a gradio app that allows you to interact with the backend. The backend directory contains the Home Assistant configuration together with a FastAPI server that allows you to interact with the Home Assistant API and the Llama model through Ollama.</p>
<h3><strong>2. Compose the backend</strong></h3>
<p>The next step is to compose the backend. This will download and start the Home Assistant container and the FastAPI server.</p>
<pre><code class="language-bash">docker compose up --build
</code></pre>
<h3><strong>3. Setting up Home Assistant</strong></h3>
<p>Once Home Assistant has started, you will be able to access the onboarding page by navigating to <em><a href="http://localhost:8123">http://localhost:8123</a></em></p>
<p>You should see a page like this:</p>
<p><img src="/llama-homeassistant-blog/assets/images/ha_onboarding.png" alt="Home Assistant onboarding"></p>
<p>Click on <strong>Create my smart home</strong> and follow the instructions.
Once you have completed the onboarding, you will see a dashboard like this:</p>
<p><img src="/llama-homeassistant-blog/assets/images/ha_dashboard.png" alt="Home Assistant dashboard"></p>
<p><strong>(Optional)</strong> The next step is optional but, since this quickstart enables only a few supported actions, we recommend editing the dashboard to display lights, an alarm panel, and a thermostat. You can do this by clicking on the top right pencil icon and then the three dots on the top right corner of the popup. Like this:</p>
<p><img src="/llama-homeassistant-blog/assets/images/ha_dashboard_customization.png" alt="Home Assistant dashboard customization"></p>
<p>Click on <strong>Take control</strong> and select <strong>Start with an empty dashboard</strong>. The UI to customize the dashboard should be intuitive; feel free to play around with it and try to add different cards. The goal is to have a dashboard that looks like this:</p>
<p><img src="/llama-homeassistant-blog/assets/images/ha_dashboard_final.png" alt="Home Assistant dashboard"></p>
<h3><strong>4. Paste the Home Assistant API token into your .env file</strong></h3>
<p>With the onboarding complete, and (hopefully) the dashboard customized, we can now issue a token for our Llama agent to interact with Home Assistant. Follow these steps:</p>
<ul>
<li>Click on your <strong>profile</strong> at the bottom left corner of the page.</li>
<li>On the new page, go to the <strong>Security</strong> tab.</li>
<li>Scroll down until you see the <strong>Long-Lived Access Tokens</strong> section.</li>
<li>Click on <strong>Create token</strong>.</li>
<li>Give the token a name and click <strong>OK</strong>.</li>
<li>Make sure you <strong>copy the token and save it somewhere</strong>, it will not be shown again.</li>
</ul>
<p><img src="/llama-homeassistant-blog/assets/images/ha_token.png" alt="Home Assistant token"></p>
<p>With the token created, in the root directory of the repo, make your own <strong>.env</strong> file based on the provided <em>.env.example</em> file and paste the token in the <strong>HOMEASSISTANT_TOKEN</strong> field.</p>
<h3><strong>5. Re-compose the backend</strong></h3>
<p>Now we are ready to restart our FastAPI server with the access token for our agent to interact with our smart home.</p>
<p><strong>Make sure to have Ollama running</strong> (the agent will request from the default port <em>:11434</em>). You need to pull the llama3.2 model first by running:</p>
<pre><code class="language-bash">ollama pull llama3.2
</code></pre>
<p>Then you can restart the backend with the updated .env file:</p>
<pre><code class="language-bash">docker compose down 
</code></pre>
<pre><code class="language-bash">docker compose up 
</code></pre>
<p>At this point our backend is ready to interact with our home. It will be waiting for requests on <em>localhost:8000</em> as a proxy to query the Llama model, parse the response and call the appropriate function (if needed) in Home Assistant.</p>
<h3><strong>6. Start the frontend</strong></h3>
<p>The frontend is a simple gradio app that will issue requests to our server and display the response from Llama after performing any necessary actions. To start the gradio app, simply run:</p>
<pre><code class="language-bash">cd frontend
python3 -m gradio app.py
</code></pre>
<p>The frontend will start on  <em><a href="http://localhost:7860">http://localhost:7860</a></em> and you should see a screen like this:</p>
<p><img src="/llama-homeassistant-blog/assets/images/gradio.png" alt="Gradio frontend"></p>
<p>Now you can send a request to the agent and see how it interacts with your smart home. You can ask things like:</p>
<ul>
<li>"Turn on the bedroom light"</li>
<li>"Set the alarm to away, the code is 1234"</li>
<li>"It's a bit cold here, can you increase the temperature to 80 degrees?"</li>
</ul>
<p>You can also ask for recipes, cleaning routines, and more.</p>
<h2><strong>Next Steps</strong></h2>
<ol>
<li>Hope you enjoyed this quickstart! The demo video shows a few example requests and provides a taste of what is possible with this integration.</li>
<li>Feel free to reach out with any questions or feedback. We encourage you to expand the functionality of the agent by adding more automations and actions on this repo:</li>
</ol>
<p><strong><a href="https://github.com/miguelg719/homeassistant-llama">https://github.com/miguelg719/homeassistant-llama</a></strong></p>
<p>Keep hacking!</p>
9:["$","main",null,{"children":["$","div",null,{"className":"container mx-auto px-5","children":["$","article",null,{"className":"mb-32","children":[["$","section",null,{"className":"mt-20 mb-6 md:mb-6","children":[["$","h1",null,{"className":"text-4xl md:text-6xl font-bold tracking-tighter leading-tight md:leading-none mb-12 text-center","children":"Llama + Home Assistant: Your local smart home assistant"}],["$","div",null,{"className":"mb-8 md:mb-16 sm:mx-0 flex justify-center","children":["$","video",null,{"controls":true,"className":"w-6/7 md:w-5/6 lg:w-4/6 rounded-lg","children":["$","source",null,{"src":"/llama-homeassistant-blog/assets/videos/smart_home_demo.mp4","type":"video/mp4"}]}]}],["$","div",null,{"className":"max-w-2xl mx-auto","children":[["$","div",null,{"className":"block mb-6","children":["$","div",null,{"className":"flex items-center","children":["$","div",null,{"className":"text-xl font-bold","children":"Miguel Gonzalez"}]}]}],["$","div",null,{"className":"mb-4 text-lg","children":["$","time",null,{"dateTime":"2020-03-16T05:35:07.322Z","children":"March\t15, 2020"}]}]]}]]}],["$","$L11",null,{"content":"$12"}]]}]}]}]
f:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
d:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Llama + Home Assistant Blog"}],["$","meta","2",{"name":"description","content":"A blog about getting started with Llama to control Home Assistant"}],["$","meta","3",{"property":"og:title","content":"Llama + Home Assistant Blog"}],["$","meta","4",{"property":"og:description","content":"A blog about getting started with Llama to control Home Assistant"}],["$","meta","5",{"property":"og:image","content":"https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png"}],["$","meta","6",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","7",{"name":"twitter:title","content":"Llama + Home Assistant Blog"}],["$","meta","8",{"name":"twitter:description","content":"A blog about getting started with Llama to control Home Assistant"}],["$","meta","9",{"name":"twitter:image","content":"https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png"}],["$","link","10",{"rel":"shortcut icon","href":"https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png"}],["$","link","11",{"rel":"icon","href":"https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png"}],["$","link","12",{"rel":"apple-touch-icon","href":"https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png"}],["$","link","13",{"rel":"apple-touch-icon","href":"https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png"}]]
b:null
