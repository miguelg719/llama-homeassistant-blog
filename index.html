<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/llama-homeassistant-blog/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/llama-homeassistant-blog/_next/static/css/e30892b19e03c492.css" data-precedence="next"/><link rel="stylesheet" href="/llama-homeassistant-blog/_next/static/css/efdab4d14d0d6b98.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/llama-homeassistant-blog/_next/static/chunks/webpack-363dcc718241cd0f.js"/><script src="/llama-homeassistant-blog/_next/static/chunks/4bd1b696-fe8bf477ee4ad839.js" async=""></script><script src="/llama-homeassistant-blog/_next/static/chunks/215-a15aeaf4f38c9597.js" async=""></script><script src="/llama-homeassistant-blog/_next/static/chunks/main-app-87ace0e1bbf2a953.js" async=""></script><script src="/llama-homeassistant-blog/_next/static/chunks/870fdd6f-e564c4dfd7fd2e96.js" async=""></script><script src="/llama-homeassistant-blog/_next/static/chunks/561-79e7d1e5757edd0e.js" async=""></script><script src="/llama-homeassistant-blog/_next/static/chunks/app/layout-1b368952c5244096.js" async=""></script><script src="/llama-homeassistant-blog/_next/static/chunks/528-59fd6bbdde9e2253.js" async=""></script><script src="/llama-homeassistant-blog/_next/static/chunks/app/page-d765ff1845a6d077.js" async=""></script><meta name="next-size-adjust"/><link rel="icon" type="image/png" href="https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png"/><meta name="msapplication-TileColor" content="#000000"/><meta name="msapplication-config" content="/favicon/browserconfig.xml"/><meta name="theme-color" content="#000"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><title>Llama + Home Assistant Blog</title><meta name="description" content="A blog about getting started with Llama to control Home Assistant"/><meta property="og:title" content="Llama + Home Assistant Blog"/><meta property="og:description" content="A blog about getting started with Llama to control Home Assistant"/><meta property="og:image" content="https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Llama + Home Assistant Blog"/><meta name="twitter:description" content="A blog about getting started with Llama to control Home Assistant"/><meta name="twitter:image" content="https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png"/><link rel="shortcut icon" href="https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png"/><link rel="icon" href="https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png"/><link rel="apple-touch-icon" href="https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png"/><link rel="apple-touch-icon" href="https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png"/><script src="/llama-homeassistant-blog/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_a3f648 dark:bg-slate-900 dark:text-slate-400"><script>(e=>{let[t,n,r]=["system","dark","light"],i=()=>{let e=document.createElement("style");return e.textContent="*,*:after,*:before{transition:none !important;}",document.head.appendChild(e),()=>{getComputedStyle(document.body),setTimeout(()=>document.head.removeChild(e),1)}},a=matchMedia(`(prefers-color-scheme: ${n})`);window.updateDOM=()=>{let s=i(),o=localStorage.getItem(e)??t,u=a.matches?n:r,l=o===t?u:o,c=document.documentElement.classList;l===n?c.add(n):c.remove(n),document.documentElement.setAttribute("data-mode",o),s()},window.updateDOM(),a.addEventListener("change",window.updateDOM)})('nextjs-blog-starter-theme')</script><button class="switch_switch__v3CFJ"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="circle-half-stroke" class="svg-inline--fa fa-circle-half-stroke w-5 h-5" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M448 256c0-106-86-192-192-192l0 384c106 0 192-86 192-192zM0 256a256 256 0 1 1 512 0A256 256 0 1 1 0 256z"></path></svg></button><div class="min-h-screen"><main><div class="container mx-auto px-5"><article class="mb-32"><section class="mt-20 mb-6 md:mb-6"><h1 class="text-4xl md:text-6xl font-bold tracking-tighter leading-tight md:leading-none mb-12 text-center">Llama + Home Assistant: Your local smart home assistant</h1><div class="mb-8 md:mb-16 sm:mx-0 flex justify-center"><video controls="" poster="/llama-homeassistant-blog/assets/images/thumbnail.png" class="w-6/7 md:w-5/6 lg:w-4/6 rounded-lg bg-gray-100 dark:bg-gray-800" preload="metadata"><source src="/llama-homeassistant-blog/assets/videos/smart_home_demo.mp4" type="video/mp4"/></video></div><div class="max-w-2xl mx-auto"><div class="block mb-6"><div class="flex items-center"><div class="text-xl font-bold">Miguel Gonzalez</div></div></div><div class="mb-4 text-lg"><time dateTime="2020-03-16T05:35:07.322Z">March	15, 2020</time></div></div></section><div class="max-w-2xl mx-auto"><div class="mb-8 flex items-center italic"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clock w-5 h-5 mr-2"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>10 minutes</div><div class="markdown-styles_markdown__ORfR8"><p>Smart home technology has revolutionized how we interact with our homes, but it comes with significant trade-offs: fragmented ecosystems, privacy concerns, and a lack of truly personalized automation. What if you could have all the convenience of a smart home assistant without sacrificing your data or control? In this quickstart we introduce a groundbreaking integration of Home Assistant and Meta’s Llama 3.2 delivering the world’s first private, local, and adaptive smart home assistant.</p>
<p>The current landscape of smart home technology faces the following challenges:</p>
<ol>
<li><strong>Fragmentation</strong>: Devices from different brands rarely play well together, forcing users to juggle multiple apps and interfaces.</li>
<li><strong>Privacy Concerns</strong>: Popular assistants rely on cloud-based processing, exposing your data to external servers and risking potential breaches.</li>
<li><strong>Lack of Personalization</strong>: While assistants can perform basic tasks, they often fail to learn and adapt to your unique needs and habits.</li>
</ol>
<p>These limitations create a frustrating user experience that prioritizes convenience over privacy and customization. Open-source frameworks like Home Assistant, a platform that connects and manages smart devices, aim to address some of these issues, but their complex setup and learning curve can discourage non-technical users from adopting them. By leveraging Meta’s Llama 3.2, an advanced large language model (LLM), available in small sizes (1B, 3B), we introduce a system that:</p>
<ol>
<li><strong>Processes Locally</strong>: All data stays on your home network, ensuring privacy and eliminating dependence on the cloud.</li>
<li><strong>Simplifies Smart Home Management</strong>: Home Assistant acts as a centralized hub, unifying control of devices across brands and ecosystems whereas the Llama model allows you to control devices and create automations using intuitive voice or text commands.</li>
<li><strong>Learns and Adapts</strong>: The system proactively suggests and implements automations based on your preferences and routines.</li>
</ol>
<h2><strong>Try it yourself</strong></h2>
<p>In this quickstart, we will guide you through the first steps of setting up the integration. We will download Home Assistant as a Docker container, run through the onboarding process, start our backend server, and finally run a Gradio frontend to interact with the Llama model.</p>
<h3><strong>Prerequisites</strong></h3>
<ul>
<li>Docker</li>
<li>Python 3.10+</li>
<li>Ollama</li>
</ul>
<h3><strong>1. Clone the repo</strong></h3>
<p>You can find the repo <strong><a href="https://github.com/miguelg719/homeassistant-llama">here</a></strong>.</p>
<pre><code class="language-bash">git clone https://github.com/miguelg719/homeassistant-llama
cd homeassistant-llama
</code></pre>
<p>Notice how the repo contains frontend/ and backend/ directories. The frontend directory contains a Gradio app that allows you to interact with the backend. The backend directory contains the Home Assistant configuration and a FastAPI server that allows you to interact with the Home Assistant API and the Llama model through Ollama.</p>
<h3><strong>2. Compose the backend</strong></h3>
<p>The next step is to compose the backend. This will download and start the Home Assistant container and the FastAPI server.</p>
<pre><code class="language-bash">docker compose up --build
</code></pre>
<h3><strong>3. Setting up Home Assistant</strong></h3>
<p>Once Home Assistant has started, you will be able to access the onboarding page by navigating to <em><a href="http://localhost:8123">http://localhost:8123</a></em></p>
<p>You should see a page like this:</p>
<p><img src="/llama-homeassistant-blog/assets/images/ha_onboarding.png" alt="Home Assistant onboarding"></p>
<p>Click on <strong>Create my smart home</strong> and follow the instructions.
Once you have completed the onboarding, you will see a dashboard like this:</p>
<p><img src="/llama-homeassistant-blog/assets/images/ha_dashboard.png" alt="Home Assistant dashboard"></p>
<p><strong>(Optional)</strong> The next step is optional but, since this quickstart enables only a few supported actions, we recommend editing the dashboard to display lights, an alarm panel, and a thermostat. You can do this by clicking on the top right pencil icon and then the three dots on the top right corner of the popup. Like this:</p>
<p><img src="/llama-homeassistant-blog/assets/images/ha_dashboard_customization.png" alt="Home Assistant dashboard customization"></p>
<p>Click on <strong>Take control</strong> and select <strong>Start with an empty dashboard</strong>. The UI to customize the dashboard should be intuitive; feel free to play around with it and try to add different cards. The goal is to have a dashboard that looks like this:</p>
<p><img src="/llama-homeassistant-blog/assets/images/ha_dashboard_final.png" alt="Home Assistant dashboard"></p>
<h3><strong>4. Paste the Home Assistant API token into your .env file</strong></h3>
<p>With the onboarding complete, and (hopefully) the dashboard customized, we can now issue a token for our Llama agent to interact with Home Assistant. Follow these steps:</p>
<ul>
<li>Click on your <strong>profile</strong> at the bottom left corner of the page.</li>
<li>On the new page, go to the <strong>Security</strong> tab.</li>
<li>Scroll down until you see the <strong>Long-Lived Access Tokens</strong> section.</li>
<li>Click on <strong>Create token</strong>.</li>
<li>Give the token a name and click <strong>OK</strong>.</li>
<li>Make sure you <strong>copy the token and save it somewhere</strong>, it will not be shown again.</li>
</ul>
<p><img src="/llama-homeassistant-blog/assets/images/ha_token.png" alt="Home Assistant token"></p>
<p>With the token created, in the root directory of the repo, make your own <strong>.env</strong> file based on the provided <em>.env.example</em> file and paste the token in the <strong>HOMEASSISTANT_TOKEN</strong> field.</p>
<h3><strong>5. Re-compose the backend</strong></h3>
<p>Now that we have the access token, we are ready to restart the FastAPI server so the agent can interact with our smart home.</p>
<p><strong>Make sure to have Ollama running</strong> (the agent will request from the default port <em>:11434</em>). Try to pull the llama3.2 model first by running:</p>
<pre><code class="language-bash">ollama pull llama3.2
</code></pre>
<p>Then you can restart the backend with the updated .env file:</p>
<pre><code class="language-bash">docker compose down 
</code></pre>
<pre><code class="language-bash">docker compose up 
</code></pre>
<p>At this point our agent is ready to interact with the home. It will be waiting for requests on <em>localhost:8000</em> as a proxy to query the Llama model, parse the generated response from Llama, and call the appropriate function (if needed) in Home Assistant.</p>
<h3><strong>6. Start the frontend</strong></h3>
<p>The frontend is a simple Gradio app that will issue requests to our server and display the response from Llama after performing any necessary actions. To start the Gradio app, simply run:</p>
<pre><code class="language-bash">cd frontend
python3 -m gradio app.py
</code></pre>
<p>The frontend will start on  <em><a href="http://localhost:7860">http://localhost:7860</a></em> and you should see a screen like this:</p>
<p><img src="/llama-homeassistant-blog/assets/images/gradio.png" alt="Gradio frontend"></p>
<p>Now you can send a request to the agent and see how it interacts with your smart home. You can ask things like:</p>
<ul>
<li>"Turn on the bedroom light"</li>
<li>"Set the alarm to away, the code is 1234"</li>
<li>"It's a bit cold here, can you increase the temperature to 80 degrees?"</li>
</ul>
<p>You can also ask for recipes, cleaning routines, and more.</p>
<h2><strong>Next Steps</strong></h2>
<ol>
<li>Hope you enjoyed this quickstart! The demo video shows a few example requests and provides a taste of what is possible with this integration.</li>
<li>Feel free to reach out with any questions or feedback, here is my <strong><a href="mailto:miguelg719@gmail.com">email</a></strong>, also happy to connect on <strong><a href="https://www.linkedin.com/in/gonzalezfernandezmiguel/">LinkedIn</a></strong>. We encourage you to expand the functionality of the agent by adding more automations and actions on this repo:</li>
</ol>
<p><strong><a href="https://github.com/miguelg719/homeassistant-llama">https://github.com/miguelg719/homeassistant-llama</a></strong></p>
<p>Keep hacking!</p>
</div></div></article></div></main></div><footer class="bg-neutral-50 border-t border-neutral-200 dark:bg-slate-800"><div class="container mx-auto px-5"><div class="py-6 flex flex-col lg:flex-row items-center justify-between"><h3 class="text-xs font-small text-center lg:text-left lg:mb-0 order-2 lg:order-1">Built with Next.js</h3><div class="flex flex-row gap-2 mb-4 lg:mb-0 items-center order-1 lg:order-2"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" class="svg-inline--fa fa-github w-5 h-5 lg:w-7 lg:h-7 text-neutral-700 dark:text-neutral-200" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><a href="https://github.com/miguelg719/llama-homeassistant-blog" class="text-sm hover:underline">GitHub</a></div></div></div></footer><script src="/llama-homeassistant-blog/_next/static/chunks/webpack-363dcc718241cd0f.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"4:\"$Sreact.fragment\"\n5:I[8126,[\"676\",\"static/chunks/870fdd6f-e564c4dfd7fd2e96.js\",\"561\",\"static/chunks/561-79e7d1e5757edd0e.js\",\"185\",\"static/chunks/app/layout-1b368952c5244096.js\"],\"ThemeSwitcher\"]\n6:I[9275,[],\"\"]\n7:I[1343,[],\"\"]\na:I[3120,[],\"OutletBoundary\"]\nc:I[3120,[],\"MetadataBoundary\"]\ne:I[3120,[],\"ViewportBoundary\"]\n10:I[6130,[],\"\"]\n1:HL[\"/llama-homeassistant-blog/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/llama-homeassistant-blog/_next/static/css/e30892b19e03c492.css\",\"style\"]\n3:HL[\"/llama-homeassistant-blog/_next/static/css/efdab4d14d0d6b98.css\",\"style\"]\n8:T518,M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"6R1bOxdqaJ-b-DOYhU6jh\",\"p\":\"/llama-homeassistant-blog\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$4\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/llama-homeassistant-blog/_next/static/css/e30892b19e03c492.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"href\":\"https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png\"}],[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#000000\"}],[\"$\",\"meta\",null,{\"name\":\"msapplication-config\",\"content\":\"/favicon/browserconfig.xml\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"content\":\"#000\"}],[\"$\",\"link\",null,{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"/feed.xml\"}]]}],[\"$\",\"body\",null,{\"className\":\"__className_a3f648 dark:bg-slate-900 dark:text-slate-400\",\"children\":[[\"$\",\"$L5\",null,{}],[\"$\",\"div\",null,{\"className\":\"min-h-screen\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}],[\"$\",\"footer\",null,{\"className\":\"bg-neutral-50 border-t border-neutral-200 dark:bg-slate-800\",\"children\":[\"$\",\"div\",null,{\"className\":\"container mx-auto px-5\",\"children\":[\"$\",\"div\",null,{\"className\":\"py-6 flex flex-col lg:flex-row items-center justify-between\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xs font-small text-center lg:text-left lg:mb-0 order-2 lg:order-1\",\"children\":\"Built with Next.js\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-row gap-2 mb-4 lg:mb-0 items-center order-1 lg:order-2\",\"children\":[[\"$\",\"svg\",null,{\"aria-hidden\":\"true\",\"focusable\":\"false\",\"data-prefix\":\"fab\",\"data-icon\":\"github\",\"className\":\"svg-inline--fa fa-github w-5 h-5 lg:w-7 lg:h-7 text-neutral-700 dark:text-neutral-200\",\"role\":\"img\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 496 512\",\"style\":{},\"ref\":\"$undefined\",\"children\":[\"$\",\"path\",null,{\"fill\":\"currentColor\",\"d\":\"$8\",\"style\":{}}]}],[\"$\",\"a\",null,{\"href\":\"https://github.com/miguelg719/llama-homeassistant-blog\",\"className\":\"text-sm hover:underline\",\"children\":\"GitHub\"}]]}]]}]}]}]]}]]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$4\",\"c\",{\"children\":[\"$L9\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/llama-homeassistant-blog/_next/static/css/efdab4d14d0d6b98.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$La\",null,{\"children\":\"$Lb\"}]]}],{},null]},null],[\"$\",\"$4\",\"h\",{\"children\":[null,[\"$\",\"$4\",\"XiIC2CD-YO_EFxq6uWbv6\",{\"children\":[[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}],[\"$\",\"$Le\",null,{\"children\":\"$Lf\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\"}]]}]]}]]],\"m\":\"$undefined\",\"G\":[\"$10\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"11:I[7067,[\"528\",\"static/chunks/528-59fd6bbdde9e2253.js\",\"931\",\"static/chunks/app/page-d765ff1845a6d077.js\"],\"PostBody\"]\n12:T217d,"])</script><script>self.__next_f.push([1,"\u003cp\u003eSmart home technology has revolutionized how we interact with our homes, but it comes with significant trade-offs: fragmented ecosystems, privacy concerns, and a lack of truly personalized automation. What if you could have all the convenience of a smart home assistant without sacrificing your data or control? In this quickstart we introduce a groundbreaking integration of Home Assistant and Meta’s Llama 3.2 delivering the world’s first private, local, and adaptive smart home assistant.\u003c/p\u003e\n\u003cp\u003eThe current landscape of smart home technology faces the following challenges:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eFragmentation\u003c/strong\u003e: Devices from different brands rarely play well together, forcing users to juggle multiple apps and interfaces.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePrivacy Concerns\u003c/strong\u003e: Popular assistants rely on cloud-based processing, exposing your data to external servers and risking potential breaches.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLack of Personalization\u003c/strong\u003e: While assistants can perform basic tasks, they often fail to learn and adapt to your unique needs and habits.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThese limitations create a frustrating user experience that prioritizes convenience over privacy and customization. Open-source frameworks like Home Assistant, a platform that connects and manages smart devices, aim to address some of these issues, but their complex setup and learning curve can discourage non-technical users from adopting them. By leveraging Meta’s Llama 3.2, an advanced large language model (LLM), available in small sizes (1B, 3B), we introduce a system that:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eProcesses Locally\u003c/strong\u003e: All data stays on your home network, ensuring privacy and eliminating dependence on the cloud.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSimplifies Smart Home Management\u003c/strong\u003e: Home Assistant acts as a centralized hub, unifying control of devices across brands and ecosystems whereas the Llama model allows you to control devices and create automations using intuitive voice or text commands.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLearns and Adapts\u003c/strong\u003e: The system proactively suggests and implements automations based on your preferences and routines.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\u003cstrong\u003eTry it yourself\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eIn this quickstart, we will guide you through the first steps of setting up the integration. We will download Home Assistant as a Docker container, run through the onboarding process, start our backend server, and finally run a Gradio frontend to interact with the Llama model.\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003ePrerequisites\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDocker\u003c/li\u003e\n\u003cli\u003ePython 3.10+\u003c/li\u003e\n\u003cli\u003eOllama\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong\u003e1. Clone the repo\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eYou can find the repo \u003cstrong\u003e\u003ca href=\"https://github.com/miguelg719/homeassistant-llama\"\u003ehere\u003c/a\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003egit clone https://github.com/miguelg719/homeassistant-llama\ncd homeassistant-llama\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNotice how the repo contains frontend/ and backend/ directories. The frontend directory contains a Gradio app that allows you to interact with the backend. The backend directory contains the Home Assistant configuration and a FastAPI server that allows you to interact with the Home Assistant API and the Llama model through Ollama.\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003e2. Compose the backend\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eThe next step is to compose the backend. This will download and start the Home Assistant container and the FastAPI server.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003edocker compose up --build\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\u003cstrong\u003e3. Setting up Home Assistant\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eOnce Home Assistant has started, you will be able to access the onboarding page by navigating to \u003cem\u003e\u003ca href=\"http://localhost:8123\"\u003ehttp://localhost:8123\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eYou should see a page like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/llama-homeassistant-blog/assets/images/ha_onboarding.png\" alt=\"Home Assistant onboarding\"\u003e\u003c/p\u003e\n\u003cp\u003eClick on \u003cstrong\u003eCreate my smart home\u003c/strong\u003e and follow the instructions.\nOnce you have completed the onboarding, you will see a dashboard like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/llama-homeassistant-blog/assets/images/ha_dashboard.png\" alt=\"Home Assistant dashboard\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e(Optional)\u003c/strong\u003e The next step is optional but, since this quickstart enables only a few supported actions, we recommend editing the dashboard to display lights, an alarm panel, and a thermostat. You can do this by clicking on the top right pencil icon and then the three dots on the top right corner of the popup. Like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/llama-homeassistant-blog/assets/images/ha_dashboard_customization.png\" alt=\"Home Assistant dashboard customization\"\u003e\u003c/p\u003e\n\u003cp\u003eClick on \u003cstrong\u003eTake control\u003c/strong\u003e and select \u003cstrong\u003eStart with an empty dashboard\u003c/strong\u003e. The UI to customize the dashboard should be intuitive; feel free to play around with it and try to add different cards. The goal is to have a dashboard that looks like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/llama-homeassistant-blog/assets/images/ha_dashboard_final.png\" alt=\"Home Assistant dashboard\"\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003e4. Paste the Home Assistant API token into your .env file\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eWith the onboarding complete, and (hopefully) the dashboard customized, we can now issue a token for our Llama agent to interact with Home Assistant. Follow these steps:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eClick on your \u003cstrong\u003eprofile\u003c/strong\u003e at the bottom left corner of the page.\u003c/li\u003e\n\u003cli\u003eOn the new page, go to the \u003cstrong\u003eSecurity\u003c/strong\u003e tab.\u003c/li\u003e\n\u003cli\u003eScroll down until you see the \u003cstrong\u003eLong-Lived Access Tokens\u003c/strong\u003e section.\u003c/li\u003e\n\u003cli\u003eClick on \u003cstrong\u003eCreate token\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eGive the token a name and click \u003cstrong\u003eOK\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eMake sure you \u003cstrong\u003ecopy the token and save it somewhere\u003c/strong\u003e, it will not be shown again.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/llama-homeassistant-blog/assets/images/ha_token.png\" alt=\"Home Assistant token\"\u003e\u003c/p\u003e\n\u003cp\u003eWith the token created, in the root directory of the repo, make your own \u003cstrong\u003e.env\u003c/strong\u003e file based on the provided \u003cem\u003e.env.example\u003c/em\u003e file and paste the token in the \u003cstrong\u003eHOMEASSISTANT_TOKEN\u003c/strong\u003e field.\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003e5. Re-compose the backend\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eNow that we have the access token, we are ready to restart the FastAPI server so the agent can interact with our smart home.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMake sure to have Ollama running\u003c/strong\u003e (the agent will request from the default port \u003cem\u003e:11434\u003c/em\u003e). Try to pull the llama3.2 model first by running:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003eollama pull llama3.2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen you can restart the backend with the updated .env file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003edocker compose down \n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003edocker compose up \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAt this point our agent is ready to interact with the home. It will be waiting for requests on \u003cem\u003elocalhost:8000\u003c/em\u003e as a proxy to query the Llama model, parse the generated response from Llama, and call the appropriate function (if needed) in Home Assistant.\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003e6. Start the frontend\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eThe frontend is a simple Gradio app that will issue requests to our server and display the response from Llama after performing any necessary actions. To start the Gradio app, simply run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003ecd frontend\npython3 -m gradio app.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe frontend will start on  \u003cem\u003e\u003ca href=\"http://localhost:7860\"\u003ehttp://localhost:7860\u003c/a\u003e\u003c/em\u003e and you should see a screen like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/llama-homeassistant-blog/assets/images/gradio.png\" alt=\"Gradio frontend\"\u003e\u003c/p\u003e\n\u003cp\u003eNow you can send a request to the agent and see how it interacts with your smart home. You can ask things like:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\"Turn on the bedroom light\"\u003c/li\u003e\n\u003cli\u003e\"Set the alarm to away, the code is 1234\"\u003c/li\u003e\n\u003cli\u003e\"It's a bit cold here, can you increase the temperature to 80 degrees?\"\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can also ask for recipes, cleaning routines, and more.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eNext Steps\u003c/strong\u003e\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eHope you enjoyed this quickstart! The demo video shows a few example requests and provides a taste of what is possible with this integration.\u003c/li\u003e\n\u003cli\u003eFeel free to reach out with any questions or feedback, here is my \u003cstrong\u003e\u003ca href=\"mailto:miguelg719@gmail.com\"\u003eemail\u003c/a\u003e\u003c/strong\u003e, also happy to connect on \u003cstrong\u003e\u003ca href=\"https://www.linkedin.com/in/gonzalezfernandezmiguel/\"\u003eLinkedIn\u003c/a\u003e\u003c/strong\u003e. We encourage you to expand the functionality of the agent by adding more automations and actions on this repo:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://github.com/miguelg719/homeassistant-llama\"\u003ehttps://github.com/miguelg719/homeassistant-llama\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eKeep hacking!\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"9:[\"$\",\"main\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"container mx-auto px-5\",\"children\":[\"$\",\"article\",null,{\"className\":\"mb-32\",\"children\":[[\"$\",\"section\",null,{\"className\":\"mt-20 mb-6 md:mb-6\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl md:text-6xl font-bold tracking-tighter leading-tight md:leading-none mb-12 text-center\",\"children\":\"Llama + Home Assistant: Your local smart home assistant\"}],[\"$\",\"div\",null,{\"className\":\"mb-8 md:mb-16 sm:mx-0 flex justify-center\",\"children\":[\"$\",\"video\",null,{\"controls\":true,\"poster\":\"/llama-homeassistant-blog/assets/images/thumbnail.png\",\"className\":\"w-6/7 md:w-5/6 lg:w-4/6 rounded-lg bg-gray-100 dark:bg-gray-800\",\"preload\":\"metadata\",\"children\":[\"$\",\"source\",null,{\"src\":\"/llama-homeassistant-blog/assets/videos/smart_home_demo.mp4\",\"type\":\"video/mp4\"}]}]}],[\"$\",\"div\",null,{\"className\":\"max-w-2xl mx-auto\",\"children\":[[\"$\",\"div\",null,{\"className\":\"block mb-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-xl font-bold\",\"children\":\"Miguel Gonzalez\"}]}]}],[\"$\",\"div\",null,{\"className\":\"mb-4 text-lg\",\"children\":[\"$\",\"time\",null,{\"dateTime\":\"2020-03-16T05:35:07.322Z\",\"children\":\"March\\t15, 2020\"}]}]]}]]}],[\"$\",\"$L11\",null,{\"content\":\"$12\"}]]}]}]}]\n"])</script><script>self.__next_f.push([1,"f:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nd:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Llama + Home Assistant Blog\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"A blog about getting started with Llama to control Home Assistant\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:title\",\"content\":\"Llama + Home Assistant Blog\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:description\",\"content\":\"A blog about getting started with Llama to control Home Assistant\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:image\",\"content\":\"https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png\"}],[\"$\",\"meta\",\"6\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:title\",\"content\":\"Llama + Home Assistant Blog\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:description\",\"content\":\"A blog about getting started with Llama to control Home Assistant\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:image\",\"content\":\"https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png\"}],[\"$\",\"link\",\"10\",{\"rel\":\"shortcut icon\",\"href\":\"https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png\"}],[\"$\",\"link\",\"11\",{\"rel\":\"icon\",\"href\":\"https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png\"}],[\"$\",\"link\",\"12\",{\"rel\":\"apple-touch-icon\",\"href\":\"https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png\"}],[\"$\",\"link\",\"13\",{\"rel\":\"apple-touch-icon\",\"href\":\"https://miguelg719.github.io/llama-homeassistant-blog/favicon/github_logo.png\"}]]\n"])</script><script>self.__next_f.push([1,"b:null\n"])</script></body></html>